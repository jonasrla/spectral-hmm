@BOOK{Bishop:2006pat,
	author = {Bishop, Christopher M.},
	title = {Pattern recognition and machine learning/},
	publisher = {Springer,},
	year = {c2006.},
	address = {New York:},
	note = {Textbook for graduates.},
	url = {http://www.loc.gov/catdir/enhancements/fy0818/2006922522-t.html},
	pages={607-610}
}

@ARTICLE{Rabiner:1989tut,
	author={Rabiner, L.R.},
	journal={Proceedings of the IEEE}, 
	title={A tutorial on hidden Markov models and selected applications in speech recognition}, 
	year={1989},
	volume={77},
	number={2},
	pages={257-286},
	doi={10.1109/5.18626}
}

@ARTICLE{Baum:1967AnIW,
  author={Leonard E. Baum and John A. Eagon},
  journal={Bulletin of the American Mathematical Society},
  title={An inequality with applications to statistical estimation for probabilistic functions of Markov processes and to a model for ecology},
  year={1967},
  volume={73},
  pages={360-363}
}

@ARTICLE{Hsu:20121460,
title = {A spectral algorithm for learning Hidden Markov Models},
journal = {Journal of Computer and System Sciences},
volume = {78},
number = {5},
pages = {1460-1480},
year = {2012},
note = {JCSS Special Issue: Cloud Computing 2011},
issn = {0022-0000},
doi = {https://doi.org/10.1016/j.jcss.2011.12.025},
url = {https://www.sciencedirect.com/science/article/pii/S0022000012000244},
author = {Daniel Hsu and Sham M. Kakade and Tong Zhang},
keywords = {Hidden Markov Models, Latent variable models, Observable operator models, Time series, Spectral algorithm, Singular value decomposition, Learning probability distributions, Unsupervised learning},
abstract = {Hidden Markov Models (HMMs) are one of the most fundamental and widely used statistical tools for modeling discrete time series. In general, learning HMMs from data is computationally hard (under cryptographic assumptions), and practitioners typically resort to search heuristics which suffer from the usual local optima issues. We prove that under a natural separation condition (bounds on the smallest singular value of the HMM parameters), there is an efficient and provably correct algorithm for learning HMMs. The sample complexity of the algorithm does not explicitly depend on the number of distinct (discrete) observations—it implicitly depends on this quantity through spectral properties of the underlying HMM. This makes the algorithm particularly applicable to settings with a large number of observations, such as those in natural language processing where the space of observation is sometimes the words in a language. The algorithm is also simple, employing only a singular value decomposition and matrix multiplications.}
}

@ARTICLE{Viterbi:1967EBFC,
  author={Viterbi, A.},
  journal={IEEE Transactions on Information Theory}, 
  title={Error bounds for convolutional codes and an asymptotically optimum decoding algorithm}, 
  year={1967},
  volume={13},
  number={2},
  pages={260-269},
  doi={10.1109/TIT.1967.1054010}}

@INCOLLECTION{ZHU:202127,
title = {Chapter 2 - The basics of natural language processing},
editor = {Chenguang Zhu},
booktitle = {Machine Reading Comprehension},
publisher = {Elsevier},
pages = {27-46},
year = {2021},
isbn = {978-0-323-90118-5},
doi = {https://doi.org/10.1016/B978-0-323-90118-5.00002-3},
url = {https://www.sciencedirect.com/science/article/pii/B9780323901185000023},
author = {Chenguang Zhu},
keywords = {Tokenization, word vector, named entity recognition, part-of-speech tagging, language model},
abstract = {One of the milestones in human civilization is the advent of language. For thousands of years, almost all human knowledge has been shared via the language medium. Natural language processing (NLP) is a discipline that uses computer science and artificial intelligence techniques to analyze and understand human languages. Among the various categories of AI, the understanding of natural language is among the most difficult tasks because of its complexity and ambiguity. With the rapid progress of machine learning, statistics, and deep learning, the field of NLP has seen many breakthroughs. This chapter will introduce basic concepts in NLP, and methods to analyze and comprehend text. We will start with the basic concept of word tokenization, and then dive into other important topics such as word vectors, linguistic tagging, and language models. All of these technologies are commonly used in machine reading comprehension.}
}

@article{Zhao2014ASL,
  title={A Sober Look at Spectral Learning},
  author={H. Zhao and Pascal Poupart},
  journal={ArXiv},
  year={2014},
  volume={abs/1406.4631}
}

@ARTICLE{Blass:2021abef4d,
doi = {10.1088/1751-8121/abef4d},
url = {https://dx.doi.org/10.1088/1751-8121/abef4d},
year = {2021},
month = {aug},
publisher = {IOP Publishing},
volume = {54},
number = {31},
pages = {315303},
author = {Andreas Blass and Yuri Gurevich},
title = {Negative probabilities: what are they for?*},
journal = {Journal of Physics A: Mathematical and Theoretical},
abstract = {An observation space   is a family of probability distributions  sharing a common sample space Ω in a consistent way. A grounding for  is a signed probability distribution  on Ω yielding the correct marginal distribution  for every i. A wide variety of quantum scenarios can be formalized as observation spaces. We describe all groundings for a number of quantum observation spaces. Our main technical result is a rigorous proof that Wigner's distribution is the unique signed probability distribution yielding the correct marginal distributions for position and momentum and all their linear combinations.}
}

@ARTICLE{Zhao:2014ASL,
  title={A Sober Look at Spectral Learning},
  author={H. Zhao and Pascal Poupart},
  journal={ArXiv},
  year={2014},
  volume={abs/1406.4631}
}

@ARTICLE{Baum:1977tb01600,
author = {Dempster, A. P. and Laird, N. M. and Rubin, D. B.},
title = {Maximum Likelihood from Incomplete Data Via the EM Algorithm},
journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
volume = {39},
number = {1},
pages = {1-22},
keywords = {maximum likelihood, incomplete data, em algorithm, posterior mode},
doi = {https://doi.org/10.1111/j.2517-6161.1977.tb01600.x},
url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1977.tb01600.x},
eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.2517-6161.1977.tb01600.x},
abstract = {Summary A broadly applicable algorithm for computing maximum likelihood estimates from incomplete data is presented at various levels of generality. Theory showing the monotone behaviour of the likelihood and convergence of the algorithm is derived. Many examples are sketched, including missing value situations, applications to grouped, censored or truncated data, finite mixture models, variance component estimation, hyperparameter estimation, iteratively reweighted least squares and factor analysis.},
year = {1977}
}
